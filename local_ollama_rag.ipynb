{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama PDF RAG Notebook\n",
    "#### Original Project Idea by @Tony Kimpkemboi on Youtube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "#Suppressing the  warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Jupyter-specific imports\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "#Set environment variable for the protobuf library to clear common errors\n",
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF loaded successfully: transformer-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "#Load the PDF\n",
    "#use transformer pdf, and this will extract the paper and pdf\n",
    "local_path = \"transformer-paper.pdf\"\n",
    "if local_path:\n",
    "    loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "    data = loader.load()\n",
    "    print(f\"PDF loaded successfully: {local_path}\")\n",
    "else:\n",
    "    print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split into 51 chunks\n"
     ]
    }
   ],
   "source": [
    "#Split text into chunks\n",
    "#this uses the RecursiveCharacterTexSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(f\"Text split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "#Create vector database\n",
    "#this will create and store the vector embeddings\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\"),\n",
    "    collection_name=\"local-rag\"\n",
    ")\n",
    "print(\"Vector database created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up LLM and Retrieval\n",
    "#### 1. I setup the LLM using the llama2 model. \n",
    "#### 2. Langchain also has a chat model designed for ollama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up local LLM model and retrieval\n",
    "local_model = \"llama2\"  \n",
    "#ollama chat model from langchain\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query prompt template\n",
    "#Define the role of the AI\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate 2\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "#Set up retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB retriever: tags=['Chroma', 'OllamaEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002B8FD563110> search_kwargs={}\n",
      "Retriever: retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002B8FD563110>, search_kwargs={}) llm_chain=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are an AI language model assistant. Your task is to generate 2\\n    different versions of the given user question to retrieve relevant documents from\\n    a vector database. By generating multiple perspectives on the user question, your\\n    goal is to help the user overcome some of the limitations of the distance-based\\n    similarity search. Provide these alternative questions separated by newlines.\\n    Original question: {question}')\n",
      "| ChatOllama(model='llama2')\n",
      "| LineListOutputParser()\n"
     ]
    }
   ],
   "source": [
    "#verify vector_Db and retriever\n",
    "print(f\"Vector DB retriever: {vector_db.as_retriever()}\")\n",
    "print(f\"Retriever: {retriever}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide context\n",
    "#RAG prompt template\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the responses will be displayed using markdown\n",
    "def chat_with_pdf(question):\n",
    "    \"\"\"\n",
    "    Chat with the PDF using the RAG chain.\n",
    "    \"\"\"\n",
    "    return display(Markdown(chain.invoke(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "The main idea of this PDF document appears to be a detailed explanation and comparison of different types of neural network layers, specifically self-attention layers, recurrent layers, and convolutional layers. The author(s) discuss the advantages and disadvantages of each layer type in terms of computational complexity, parallelization capabilities, and positional encoding techniques. The document also includes figures and examples to illustrate the different attention mechanisms and their applications in sequence transduction tasks. Overall, the main idea of this PDF document is to provide a comprehensive understanding of these neural network layer types and their uses in natural language processing tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 1\n",
    "chat_with_pdf(\"What is the main idea of this PDF document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! A Transformer is a type of neural network architecture that is specifically designed for sequence modeling and transduction tasks, such as language modeling and machine translation. It was introduced in a research paper titled \"Attention is All You Need\" by Vaswani et al. in 2017.\n",
       "\n",
       "The Transformer model is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. It consists of an encoder and a decoder, each comprised of multiple identical layers, where each layer exhibits self-attention and feed-forward processing. The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance when computing the output.\n",
       "\n",
       "The Transformer has several key advantages over traditional recurrent neural network (RNN) architectures:\n",
       "\n",
       "1. Parallelization: The self-attention mechanism allows for parallelization, making it much faster and more scalable than RNNs.\n",
       "2. Efficiency: The Transformer requires fewer parameters and computations compared to RNNs, making it more efficient in terms of both training time and memory usage.\n",
       "3. Flexibility: The Transformer can handle input sequences of arbitrary length, without the need for segmentation or padding, which is a common challenge in sequence modeling tasks.\n",
       "4. Quality: The Transformer has been shown to achieve state-of-the-art results in various sequence modeling tasks, such as language modeling and machine translation.\n",
       "\n",
       "In summary, the Transformer is a powerful neural network architecture that leverages self-attention mechanisms to efficiently process sequential data, making it particularly well-suited for natural language processing tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example 2\n",
    "chat_with_pdf(\"Can you tell me what a Transformer is?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Multihead attention is a mechanism introduced in the Transformer model (a popular deep learning architecture for natural language processing tasks) that allows the model to jointly attend to information from different representation subspaces at different positions.\n",
       "\n",
       "In traditional attention mechanisms, a single attention function is applied to the query, keys, and values, and the output is computed as a weighted sum of these vectors. However, this can lead to the \"attention collapse\" problem, where the model only focuses on a limited subset of the input sequence and neglects the rest.\n",
       "\n",
       "To address this issue, Multihead attention introduces multiple attention functions (or heads) that operate in parallel, each with its own set of weights. The outputs of these heads are then combined to form the final output. This allows the model to capture different aspects of the input sequence simultaneously and avoid the collapse problem.\n",
       "\n",
       "Formally, given a query q and a set of keys k, Multihead attention computes the output o as follows:\n",
       "\n",
       "o = Concat(h1, ..., hh) * W^O\n",
       "\n",
       "where h1, ..., hh are the outputs of the multiple attention heads, and W^O is a learnable weight matrix. The Concatenation operator (Concat) combines the outputs of the different heads without any additional processing.\n",
       "\n",
       "Multihead attention allows the model to capture different contextual relationships in the input sequence, leading to better performance in tasks such as machine translation, question answering, and text classification."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example 3\n",
    "chat_with_pdf(\"Explain to me what Multihead attention is?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The paper \"Attention Is All You Need\" by Ashish Vaswani et al. (2017) introduced a new neural network architecture for machine translation called the Transformer model, which relies entirely on self-attention mechanisms instead of traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs). The Transformer model achieved state-of-the-art results in machine translation tasks, outperforming other state-of-the-art models of the time.\n",
       "\n",
       "Specifically, the paper reported the following results:\n",
       "\n",
       "1. The Transformer model achieved a BLEU score of 28.7 on the English-German translation task, which was a significant improvement over the previous state-of-the-art score of 25.3.\n",
       "2. The Transformer model also achieved a ROUGE score of 40.6 on the English-French translation task, which was an improvement over the previous state-of-the-art score of 37.\n",
       "3. The Transformer model was able to handle long-range dependencies in the input sequence more effectively than other models, as demonstrated by its performance on the WMT17 machine translation competition.\n",
       "4. The Transformer model required significantly fewer parameters than other state-of-the-art models, which made it more computationally efficient and easier to train.\n",
       "\n",
       "Overall, the results of the paper demonstrated the effectiveness of the Transformer model in machine translation tasks and its ability to handle long-range dependencies in input sequences. The Transformer model has since become a widely-used architecture in natural language processing tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example 4\n",
    "chat_with_pdf(\"What were the results of this paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "According to the paper, the Adam optimizer was used during training. Specifically, the authors used the Adam optimizer with the following parameters:\n",
       "\n",
       "* β1 = 0.9\n",
       "* β2 = 0.98\n",
       "* ε = 10^-9\n",
       "\n",
       "They also varied the learning rate over time according to a specific formula, which is mentioned in the paper."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example 5\n",
    "chat_with_pdf(\"What optimizer was used during training?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database deleted successfully\n"
     ]
    }
   ],
   "source": [
    "#Optional: Clean up the vector database when done \n",
    "vector_db.delete_collection()\n",
    "print(\"Vector database deleted successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
